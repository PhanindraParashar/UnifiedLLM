{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unifiedllm import AsyncLLM,ProcessOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expects API keys to be saved in .env file\n",
    "- PERPLEXITY_API_KEY\n",
    "- OPENAI_API_KEY \n",
    "- ANTHROPIC_API_KEY \n",
    "- DEEPSEEK_API_KEY \n",
    "- GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize The Async LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allm = AsyncLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending 2 requests in async mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is the capital of France?\", \"What is the capital of Germany?\"]\n",
    "system_messages = [\"You are a helpful assistant and answer in less than 10 words.\" for _ in range(len(prompts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the results of the requests using await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = await allm.batch_chat_complete(prompts=prompts,system_messages=system_messages,model=\"llama-3.2-1b-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response is a list of tuples, each tuple contains text output and the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris.', 2.3600000000000003e-06),\n",
       " ('Berlin is the capital of Germany.', 2.56e-06)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing LLM output to get json output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processing = ProcessOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is the capital of France?\", \"What is the capital of Germany?\"]\n",
    "system_messages = [\"You are a helpful assistant and answer only in json format {'question':question,'answer':1 word answer}\" for _ in range(len(prompts))]\n",
    "\n",
    "responses = await allm.batch_chat_complete(prompts=prompts,system_messages=system_messages,model=\"llama-3.2-1b-preview\")\n",
    "answers = [i[0] for i in responses]\n",
    "cost = [i[1] for i in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using jsonify_llm_new method to extract json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the capital of France?', 'answer': 'Paris'},\n",
       " {'question': 'What is the capital of Germany?', 'answer': 'Berlin'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Processing.jsonify_llm_new(i) for i in answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Json within a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is the capital of France?\", \"What is the capital of Germany?\"]\n",
    "system_messages = [\"You are a helpful assistant and answer only in json format <answer>{'question':question,'answer':1 word answer}</answer> within the <answer> and </answer> tags\" for _ in range(len(prompts))]\n",
    "\n",
    "responses = await allm.batch_chat_complete(prompts=prompts,system_messages=system_messages,model=\"llama-3.2-3b-preview\")\n",
    "answers = [i[0] for i in responses]\n",
    "cost = [i[1] for i in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<answer>{ 'question': 'What is the capital of France?', 'answer': 'Paris' }</answer>\",\n",
       " \"<answer>{ 'question': 'What is the capital of Germany?', 'answer': 'Berlin' }</answer>\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the capital of France?', 'answer': 'Paris'},\n",
       " {'question': 'What is the capital of Germany?', 'answer': 'Berlin'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Processing.jsonify_llm(i,tag_name=\"answer\") for i in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrag311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
